{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgk2IL4S4-Tl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlqGvjdL6tFE"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLBJFavuaLXn"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gZcxKxWV_Mh4",
        "outputId": "6661d3d5-587f-4110-bee5-f052cf4b81b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a056b3ed-30ea-425d-b07d-72eb4011d0c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>negative</td>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>negative</td>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>negative</td>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>negative</td>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4846 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a056b3ed-30ea-425d-b07d-72eb4011d0c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a056b3ed-30ea-425d-b07d-72eb4011d0c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a056b3ed-30ea-425d-b07d-72eb4011d0c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             0                                                  1\n",
              "0      neutral  According to Gran , the company has no plans t...\n",
              "1      neutral  Technopolis plans to develop in stages an area...\n",
              "2     negative  The international electronic industry company ...\n",
              "3     positive  With the new production plant the company woul...\n",
              "4     positive  According to the company 's updated strategy f...\n",
              "...        ...                                                ...\n",
              "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
              "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
              "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
              "4844  negative  Net sales of the Paper segment decreased to EU...\n",
              "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
              "\n",
              "[4846 rows x 2 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sheet_url = 'https://docs.google.com/spreadsheets/d/1IG7APAMDUOBfToE_NDM9uKczkIUNC5__d9BsEK1DMIY/edit#gid=1880462315'\n",
        "url_1 = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "df = pd.read_csv(url_1, header=None)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p4Y79Fi2i2V"
      },
      "source": [
        "### Paraphrase Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvHmSGu-NEVi"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0kymSkfM4jT",
        "outputId": "74aa5471-c1bf-49c4-8227-5da48ced4fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paraphrase 1: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def generate_paraphrase(text, model, tokenizer, num_return_sequences=1):\n",
        "    input_text = \"paraphrase: \" + text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    outputs = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=256,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "      top_k=120,\n",
        "      top_p=0.95,\n",
        "      temperature=0.9,\n",
        "    )\n",
        "\n",
        "    paraphrases = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return paraphrases\n",
        "\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "text = \"Stock prices soared after the company announced better-than-expected earnings.\"\n",
        "paraphrases = generate_paraphrase(text, model, tokenizer, num_return_sequences=1)\n",
        "\n",
        "for i, paraphrase in enumerate(paraphrases):\n",
        "    print(f\"Paraphrase {i + 1}: {paraphrase}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI_ZaLhT2nsO"
      },
      "source": [
        "### Paraphrase Augmentation (II)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zetfbuJ32cZJ",
        "outputId": "63de2b12-8f55-4e16-fad1-b1f587c24709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def generate_phrases(text, num_phrases=1):\n",
        "    model_name = 't5-base'\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, num_return_sequences=num_phrases, max_length=256, num_beams=10, temperature=0.6)\n",
        "\n",
        "    phrases = []\n",
        "    for output in outputs:\n",
        "        phrase = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        phrases.append(phrase)\n",
        "\n",
        "    return phrases\n",
        "\n",
        "text = \"I don't like eating chocolate that tastes average, as it is a waste of calories. I apply this to other food too.\"\n",
        "paraphrased_phrases = generate_phrases(text, num_phrases=1)\n",
        "\n",
        "for i, phrase in enumerate(paraphrased_phrases):\n",
        "    print(f\"{i+1}: {phrase}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvPPB13o38vn",
        "outputId": "b9bc6ce2-520d-4a20-e0f4-09f0bc2f5add"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Paraphrased: False\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def paraphrase_sentence(sentence, model_name='t5-base', max_length=315):\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    text = f\"paraphrase: {sentence}\"\n",
        "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding['input_ids'], \n",
        "        attention_mask=encoding['attention_mask'], \n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        num_beams=10,\n",
        "        temperature=0.8,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return paraphrased_text\n",
        "\n",
        "input_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "paraphrased = paraphrase_sentence(input_sentence)\n",
        "print(f\"Original: {input_sentence}\")\n",
        "print(f\"Paraphrased: {paraphrased}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry-7fuw-52Ia",
        "outputId": "0a29e69d-3a56-4b97-f954-2e469ec36bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
            "Paraphrased: Gran\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def paraphrase_sentence(sentence, model_name='t5-large', max_length=315):\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    text = f\"paraphrase: {sentence}\"\n",
        "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "    outputs = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], max_length=max_length)\n",
        "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return paraphrased_text\n",
        "\n",
        "input_sentence = \"According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\"\n",
        "paraphrased = paraphrase_sentence(input_sentence)\n",
        "print(f\"Original: {input_sentence}\")\n",
        "print(f\"Paraphrased: {paraphrased}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaOK1sTx2tks"
      },
      "source": [
        "### Synonym Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEggYTXBD8rh",
        "outputId": "5622843e-5fb6-468f-824d-5de1a7284aa3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return set(synonyms)\n",
        "\n",
        "def replace_with_synonyms(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    new_text = []\n",
        "    for word in words:\n",
        "        synonyms = get_synonyms(word)\n",
        "        if synonyms:\n",
        "            new_text.append(synonyms.pop())\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return ' '.join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tliwclTxEhSz"
      },
      "outputs": [],
      "source": [
        "texts_aug = []\n",
        "labels_aug = []\n",
        "for i in range(len(texts)):\n",
        "  texts_aug.append(replace_with_synonyms(texts[i]))\n",
        "  labels_aug.append(labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4UFnY6mG61Z",
        "outputId": "71551610-1a92-45fd-b391-db591429af7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from non-augmentated dataset:\n",
            "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
            "neutral\n",
            "from augmentated dataset:\n",
            "grant to gran , the caller take_in no architectural_plan to make_a_motion entirely production to Russia , although that constitute where the caller constitute uprise .\n",
            "neutral\n"
          ]
        }
      ],
      "source": [
        "print('from non-augmentated dataset:')\n",
        "print(texts[0])\n",
        "print(labels[0])\n",
        "print('from augmentated dataset:')\n",
        "print(texts[4846])\n",
        "print(labels[4846])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6plUD1G7d7z"
      },
      "source": [
        "### Language Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ALskCfW7g57"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "def augment_data(texts, device=0):\n",
        "    # Load the translation pipelines for English to French and French to English\n",
        "    en_to_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=device)\n",
        "    fr_to_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=device)\n",
        "\n",
        "    # Translate the input English text to French\n",
        "    french_texts = en_to_fr(texts, max_length=315)\n",
        "\n",
        "    # Extract the translated French text\n",
        "    french_texts = [text[\"translation_text\"] for text in french_texts]\n",
        "\n",
        "    # Translate the French text back to English\n",
        "    backtranslated_texts = fr_to_en(french_texts, max_length=315)\n",
        "\n",
        "    # Extract the backtranslated English text\n",
        "    backtranslated_texts = [text[\"translation_text\"] for text in backtranslated_texts]\n",
        "\n",
        "    return backtranslated_texts\n",
        "\n",
        "\n",
        "\n",
        "# text = \"According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\"\n",
        "# augmented_text = augment_data(text)\n",
        "# print(augmented_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2_l_JHOAv_e"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def augment_data_batch(text, batch_size=10):\n",
        "    # Load the translation pipelines for English to French and French to English\n",
        "    en_to_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "    fr_to_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "    \n",
        "    # Split the input text into batches of the specified size\n",
        "    batches = [text[i:i+batch_size] for i in range(0, len(text), batch_size)]\n",
        "    \n",
        "    # Translate each batch from English to French, then from French back to English\n",
        "    translated_batches = []\n",
        "    for batch in batches:\n",
        "        french_batch = en_to_fr(batch, max_length=315, padding=True, truncation=True)\n",
        "        backtranslated_batch = fr_to_en(french_batch, max_length=315, padding=True, truncation=True)\n",
        "        translated_batches.append(backtranslated_batch)\n",
        "    \n",
        "    # Combine the translated batches into a single string and return it\n",
        "    translated_text = \"\"\n",
        "    for batch in translated_batches:\n",
        "        for translation in batch:\n",
        "            translated_text += translation[\"translation_text\"]\n",
        "    \n",
        "    return translated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDSaE19f-YZ9"
      },
      "outputs": [],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhG912-x8_bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "texts_aug = []\n",
        "labels_aug = []\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# for i in range(len(X_train)):\n",
        "#   print(i)\n",
        "#   texts_aug.append(augment_data(X_train[i]))\n",
        "#   labels_aug.append(t_train[i])\n",
        "texts_aug = augment_data(X_train.tolist())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/CSC413/Project/aug_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump([texts_aug, t_train], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVymXzQG9G3C"
      },
      "outputs": [],
      "source": [
        "texts = np.concatenate([texts, texts_aug])\n",
        "labels = np.concatenate([labels, labels_aug])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ioduRyEpvL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Create two numpy arrays\n",
        "arr1 = np.array([1, 2, 3])\n",
        "arr2 = np.array([4, 5, 6])\n",
        "\n",
        "# Create a list that contains both arrays\n",
        "data = [arr1, arr2]\n",
        "\n",
        "# Dump the list to a pickle file\n",
        "with open(\"/content/drive/MyDrive/CSC413/Project/aug_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Xt4YwfElAF",
        "outputId": "f2b9a99a-dc65-4987-b33e-6af5cb086562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([1, 2, 3]), array([4, 5, 6])]\n"
          ]
        }
      ],
      "source": [
        "# load from the pickle file\n",
        "with open(\"/content/drive/MyDrive/CSC413/Project/aug_data.pkl\", \"rb\") as f:\n",
        "    np_array_loaded = pickle.load(f)\n",
        "\n",
        "print(np_array_loaded) # [1 2 3 4 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny57M84C9PxN"
      },
      "outputs": [],
      "source": [
        "print('from non-augmentated dataset:')\n",
        "print(texts[1])\n",
        "print(labels[1])\n",
        "print('from augmentated dataset:')\n",
        "print(texts[4847])\n",
        "print(labels[4847])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
